{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aec7c20-86ef-42de-ab79-80989651f11d",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506aeff-6a0f-4ad3-9ac4-088484731bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import itertools\n",
    "import math\n",
    "import ndsafir\n",
    "import numpy as np\n",
    "from pylibCZIrw import czi\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3c078-dcfc-404e-8013-8385c7986604",
   "metadata": {},
   "source": [
    "A `dask.config` parameter that needs to be set early.  It prevents dask\n",
    "from eagerly loading extra tasks onto workers.  The default is 1.1; when\n",
    "computing how many tasks a worker should have Dask uses\n",
    "`ceil(worker-saturation * n_threads)`, so even with 1 thread Dask\n",
    "will try to load 2 tasks onto a worker.\n",
    "\n",
    "For many workloads this is reasonable, since the worker can overlap\n",
    "disk access (say) with computation.  For us this is problematic as our\n",
    "workloads do not sleep (so the other tasks get no CPU) and are very\n",
    "memory hungry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c67a6-8ff4-4fe0-80ca-d0e356b29840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dask.config.set({\"distributed.scheduler.worker-saturation\": 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46e172-2462-4024-a7a2-8818517e8743",
   "metadata": {},
   "source": [
    "We create a cluster here for testing purposes.  When running in an HPC job\n",
    "you do not want these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa506d4-325d-4b01-b1f3-ac7558407d58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1, threads_per_worker=1, memory_limit=\"16GiB\")\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f6fee-fb2f-467d-91d6-e326c62e0d0d",
   "metadata": {},
   "source": [
    "## Some utility functions and types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d882e-82a5-4a8e-b7f5-fc3fa9c84e39",
   "metadata": {},
   "source": [
    "Helper classes to hold things (indices, slices, ranges)\n",
    "corresponding to T, C, Z, Y, X, or just T, Z, Y, X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5495feaf-f39d-4d91-8dcb-11cccce82d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TZYX = collections.namedtuple(\"TZYX\", [\"T\", \"Z\", \"Y\", \"X\"])\n",
    "class TCZYX(collections.namedtuple(\"TCZYX\", [\"T\", \"C\", \"Z\", \"Y\", \"X\"])):\n",
    "    @property\n",
    "    def TZYX(self):\n",
    "        return TZYX(self.T, self.Z, self.Y, self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76099f36-0229-420a-882d-e635b42a64d5",
   "metadata": {},
   "source": [
    "A general method to determine whether a range\n",
    "is wholly contained within another.\n",
    "E.g.\n",
    "```python\n",
    ">>> r1 = range(0, 10)\n",
    ">>> r2 = range(1, 3)\n",
    ">>> range_contains(r2, r1)\n",
    "True\n",
    ">>> range_contains(r1, r2)\n",
    "False\n",
    ">>> r3 = range(4, 11)\n",
    ">>> range_contains(r3, r1)\n",
    "False\n",
    ">>> r4 = range(1, 3, -1)\n",
    ">>> range_contains(r4, r1)\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabb951-1e86-4e2f-8074-d9e17770f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_contains(r_inner: range, r_outer: range):\n",
    "    \"\"\"\n",
    "    Determines whether r_inner is entirely present in r_outer.\n",
    "    Both parameters must be ranges.\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty set is always contained.\n",
    "    if len(r_inner) == 0:\n",
    "        return True\n",
    "\n",
    "    # Inner start must be there.\n",
    "    if r_inner.start not in r_outer:\n",
    "        return False\n",
    "    if len(r_inner) == 1:\n",
    "        return True\n",
    "\n",
    "    # Actual last element of r_inner, must be there.\n",
    "    if r_inner[-1] not in r_outer:\n",
    "        return False\n",
    "    if len(r_inner) == 2:\n",
    "        return True\n",
    "\n",
    "    return (r_inner.step % r_outer.step) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b9895-cf8c-49e3-8e82-f18c9b07abed",
   "metadata": {},
   "source": [
    "Trims off any overlap from a block. This is delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd36c0-7918-434f-bfd7-b832e1afd7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def trim(block, ch_index, chunk_array, overlaps):\n",
    "    \"\"\"\n",
    "    Trim off the overlap data from the block.\n",
    "\n",
    "    Params:\n",
    "    block: np.ndarray The data to trim.\n",
    "    ch_index: TZYX The position in the overall array, in chunk units.\n",
    "    chunk_array: TZYX The size of the overall array, in chunk units.\n",
    "    overlap: TZYX The size of the overlaps in each axis.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Trimmed data.\n",
    "    \"\"\"\n",
    "    axes = []\n",
    "    for i, c_i in enumerate(ch_index):\n",
    "        lo = overlaps[i] if c_i > 0 else 0\n",
    "        hi = -overlaps[i] if overlaps[i] > 0 and c_i < chunk_array[i]-1 else None\n",
    "        axes.append(slice(lo, hi))\n",
    "    print(axes)\n",
    "    axes.insert(1, slice(0, None)) # C\n",
    "    return block[*axes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f959198-49bd-4f69-a059-0387777c4037",
   "metadata": {},
   "source": [
    "Save a block to a Zarr file. This works out where in the zarr file this chunk belongs\n",
    "then slices it directly into the Zarr array.  The file is (re-)opened for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a64814-d0aa-48b1-9256-b44a33898dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def save_block_to_zarr(block, zarr_filename, chunk_index, chunk_sizes):\n",
    "    \"\"\"\n",
    "    Save the block to output zarr array.\n",
    "\n",
    "    Params:\n",
    "    block: np.ndarray Block to save.\n",
    "    zarr_filename: str Filename of zarr array.\n",
    "    chunk_index: TZYX The position in the overall array, in chunk units.\n",
    "    chunk_size: TZYX Chunk size.\n",
    "\n",
    "    Returns:\n",
    "    nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # for now assume zarr_filename exists and is a suitable zarr array\n",
    "    output_zarr = zarr.convenience.open(zarr_filename, mode=\"r+\")\n",
    "    section = list(map(lambda t: slice(t[0] * t[1], t[0] * t[1] + t[2]), zip(chunk_index, chunk_sizes, TCZYX(*block.shape).TZYX)))\n",
    "    section.insert(1, slice(0, None))\n",
    "    dask.distributed.print(f\"Saving section {section}\")\n",
    "    output_zarr[*section] = block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b30f2a-fdd1-41f9-b2d2-daea99b6cb4f",
   "metadata": {},
   "source": [
    "The Zarr file might already contain chunks that we've computed. We want to avoid\n",
    "re-computing them. This function compares the Dask chunk (`da_chunk`) index\n",
    "to the `existing_chunks` set which contains all existing Zarr chunks.\n",
    "We assume the Dask chunk size is an integral multiple of the Zarr\n",
    "chunk size in all axes. (This is checked elsewhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a82804-df12-47a2-b196-255c224ab254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existing_chunks(da_chunk: TZYX, chunk_ratios, existing_chunks):\n",
    "    \"\"\"\n",
    "    Check to see if all necessary chunks are present in existing_chunks\n",
    "    (a set of tuples of TZYX).  It checks all zarr chunks that contribute\n",
    "    to a dask chunk.  It stops early and uses a set for checking.\n",
    "\n",
    "    The chunk ratios must be integral.\n",
    "\n",
    "    da_chunk: TZYX is the dask chunk\n",
    "    chunk_ratios: TZYX dask_chunk_size / zarr_chunk size for each axis\n",
    "    existing_chunks: set(TZYX) set of existing chunks\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    for da_ch, ch_r in zip(da_chunk, chunk_ratios):\n",
    "        ranges.append(range(da_ch * ch_r, (da_ch+1) * ch_r))\n",
    "    return all(\n",
    "        map(\n",
    "            lambda t: t in existing_chunks,\n",
    "            itertools.product(*ranges)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f61b141-61ac-4733-aecc-26b6c85a7192",
   "metadata": {},
   "source": [
    "Loads a single chunk spanning the given ranges from a CZI file. Also delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb990e5-abcb-4514-8ea2-6d741588d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_chunk(input_filepath, t_range, z_range, y_range, x_range, scene):\n",
    "    \"\"\"\n",
    "    Load slices of the input file, as given by the ranges\n",
    "    from scene `scene`.\n",
    "\n",
    "    Params:\n",
    "    input_filepath: filepath of the CZI file.\n",
    "    t_range, etc: ranges detailing region of image to load.\n",
    "    scene: CZI scene number\n",
    "\n",
    "    The ranges should be range objects (e.g. `range(0, 10)`\n",
    "    \n",
    "    This uses the extremely clunky interface of pylibCZIrw.\n",
    "    There might be cleaner alternatives in `bioio`.\n",
    "    \n",
    "    Returns a ndarray copy of the slice, in TCZYX order.\n",
    "    \"\"\"\n",
    "    with czi.open_czi(input_filepath) as input_file:\n",
    "        bbox = input_file.total_bounding_box\n",
    "        C = bbox[\"C\"][1] - bbox[\"C\"][0]\n",
    "        # The ROI can go beyond our scene without error.\n",
    "        # We will check here and raise an error if we do so.\n",
    "        bound_rect = input_file.scenes_bounding_rectangle.get(scene, None)\n",
    "        if not bound_rect:\n",
    "            bound_rect = input_file.total_bounding_rectangle\n",
    "        X = range(bound_rect[0], bound_rect[0] + bound_rect[2])\n",
    "        Y = range(bound_rect[1], bound_rect[1] + bound_rect[3])\n",
    "        if not range_contains(y_range, Y) or not range_contains(x_range, X):\n",
    "            raise ValueError(\"input range lies outside image\")\n",
    "        \n",
    "        # Can query input_file.pixel_types but assume uint16\n",
    "        data = np.empty(shape=(len(t_range), C, len(z_range), len(y_range), len(x_range)), dtype=np.uint16)\n",
    "        roi = (x_range.start, y_range.start, len(x_range), len(y_range))\n",
    "        ndarray_t = 0\n",
    "        for t in t_range:\n",
    "            ndarray_z = 0\n",
    "            for z in z_range:\n",
    "                # NB input_file zero indexed\n",
    "                data[ndarray_t, :, ndarray_z, :, :] = np.moveaxis(\n",
    "                    input_file.read(\n",
    "                        plane={\"T\": t, \"Z\": z},\n",
    "                        scene=scene,\n",
    "                        roi=roi,\n",
    "                    ),\n",
    "                    2,\n",
    "                    0\n",
    "                )\n",
    "                ndarray_z += 1\n",
    "            ndarray_t += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd73ac-07eb-4233-b25b-bd55b7b58914",
   "metadata": {},
   "source": [
    "A wrapper to call the ndsafir denoising code, with some set parameters. These could be made parameters of\n",
    "`process_block` if desired. Delayed.\n",
    "\n",
    "Note that is is important that if the code here is outside Python and is expected\n",
    "to be long-running then you should make sure the Python GIL is released before\n",
    "starting execution (e.g. for\n",
    "[Pybind](https://pybind11.readthedocs.io/en/stable/advanced/misc.html)).\n",
    "Failing to do so will usually result in Dask being unable to contact\n",
    "the Worker to check its status.  It will then kill the Worker and reassign\n",
    "the task (which will kill the next Worker, and so on).\n",
    "\n",
    "For example in C++ this might be as simple as wrapping the long-running\n",
    "function `filter.run` in a scope with a `gil_scoped_release` object.\n",
    "```\n",
    "  {\n",
    "    py::gil_scoped_release release;\n",
    "    filter.run(f, noise_std);\n",
    "  }\n",
    "```\n",
    "Obviously the external code should not access any Python objects while\n",
    "not holding the GIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb9963-275f-4ffc-ba59-891aba6c5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def process_block(block):\n",
    "    \"\"\"\n",
    "    Denoise a block, then return it\n",
    "    \"\"\"\n",
    "    denoised = ndsafir.denoise(\n",
    "        block,\n",
    "        mode=\"poisson-gaussian\",\n",
    "        gains=[3.92],\n",
    "        offsets=[-388],\n",
    "        patch=[0, 0, 1, 3, 3],\n",
    "        max_iter=4,\n",
    "        pvalue=0.1,\n",
    "        nthreads=12,\n",
    "        axes=\"TCZYX\",\n",
    "    )\n",
    "    return denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305c5f1-ac38-4a38-8c7e-2f589c8e444a",
   "metadata": {},
   "source": [
    "The `pylibCZIrw` interface is awful. The CZI file doesn't have to have\n",
    "any scenes declared, in which case the `scenes_bounding_rectangle`\n",
    "method will fail. This routine is here to return the shape\n",
    "of the scene requested whether or not there's a scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4eefb-32c3-4ba1-ae9b-37997c4bc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_czi_scene_shape(filename, scene=0):\n",
    "    \"\"\"\n",
    "    Return a TCZYX containing the sizes of the CZI file in\n",
    "    those axes.\n",
    "    \"\"\"\n",
    "    with czi.open_czi(filename) as input_file:\n",
    "        bbox = input_file.total_bounding_box\n",
    "        C = bbox[\"C\"][1] - bbox[\"C\"][0]\n",
    "        T = bbox[\"T\"][1] - bbox[\"T\"][0]\n",
    "        Z = bbox[\"Z\"][1] - bbox[\"Z\"][0]\n",
    "\n",
    "        bound_rect = input_file.scenes_bounding_rectangle.get(scene, None)\n",
    "        if not bound_rect:\n",
    "            bound_rect = input_file.total_bounding_rectangle\n",
    "        X_min = bound_rect.x\n",
    "        width = bound_rect.w\n",
    "        Y_min = bound_rect.y\n",
    "        height = bound_rect.h\n",
    "    return TCZYX(T, C, Z, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2cb61-15ea-4e53-a778-71b0da673b95",
   "metadata": {},
   "source": [
    "The main routine to load all the chunks from the CZI file. This now includes the\n",
    "overlaps in the chunks loaded. It also calls `check_existing_chunks` to\n",
    "see if all the Zarr chunks that represent each Dask chunk are present\n",
    "in the existing Zarr array.  If so then this Dask chunk is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26efac4-66ae-4169-8985-2b28c51f4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_from_czi(filename, chunk_sizes: TZYX, overlap: TZYX,\n",
    "                         existing_chunks, chunk_ratios, scene=0):\n",
    "    \"\"\"\n",
    "    A function that takes a filename and a chunking regimen and returns dict\n",
    "    made up of chunks of the appropriate size.  The chunks will include\n",
    "    overlaps as per `overlap`.\n",
    "\n",
    "    Checks to see if each chunk is wholly present in the existing chunks.\n",
    "\n",
    "    Params:\n",
    "    filename: CZI file\n",
    "    chunk_sizes: TZYX Contains chunk dimensions in those axes.\n",
    "    overlap: TZYX Contains overlap dimensions.\n",
    "    scene: CZI scene to load.\n",
    "    existing_chunks: set(TZYX) Chunks present in zarr file.\n",
    "    chunk_ratios: TZYX Ratio of dask chunk / zarr chunk sizes per axis.\n",
    "                       Must be integral.\n",
    "\n",
    "    Returns:\n",
    "    dict[(T_c, Z_c, Y_c, X_c)] = block, where T_c, etc, are indices in the\n",
    "        overall array in chunk units.\n",
    "    \"\"\"\n",
    "    with czi.open_czi(filename) as input_file:\n",
    "        bbox = input_file.total_bounding_box\n",
    "        C = bbox[\"C\"][1] - bbox[\"C\"][0]\n",
    "        T = bbox[\"T\"][1] - bbox[\"T\"][0]\n",
    "        Z = bbox[\"Z\"][1] - bbox[\"Z\"][0]\n",
    "\n",
    "        bound_rect = input_file.scenes_bounding_rectangle.get(scene, None)\n",
    "        if not bound_rect:\n",
    "            bound_rect = input_file.total_bounding_rectangle\n",
    "        X_min = bound_rect.x\n",
    "        width = bound_rect.w\n",
    "        Y_min = bound_rect.y\n",
    "        height = bound_rect.h\n",
    "\n",
    "        chunk_dict = {}\n",
    "        t_ind = 0\n",
    "        for t in range(0, T, chunk_sizes.T):\n",
    "            z_ind = 0\n",
    "            for z in range(0, Z, chunk_sizes.Z):\n",
    "                y_ind = 0\n",
    "                for y in range(Y_min, Y_min + height, chunk_sizes.Y):\n",
    "                    x_ind = 0\n",
    "                    for x in range(X_min, X_min + width, chunk_sizes.X):\n",
    "                        t_range = range(max(t - overlap.T, 0), min(T, t + chunk_sizes.T + overlap.T))\n",
    "                        z_range = range(max(z - overlap.Z, 0), min(Z, z + chunk_sizes.Z + overlap.Z))\n",
    "                        y_range = range(max(y - overlap.Y, Y_min), min(Y_min + height, y + chunk_sizes.Y + overlap.Y))\n",
    "                        x_range = range(max(x - overlap.X, X_min), min(X_min + width, x + chunk_sizes.X + overlap.X))\n",
    "                        #print(\"ranges: \",\n",
    "                        #    [f\"{r}({len(r)})\" for r in (t_range, z_range, y_range, x_range)]\n",
    "                        #)\n",
    "#                        chunk_dict[t_idx, 0, z_idx, y_idx, x_idx] = load_chunk(\n",
    "\n",
    "                        # Check output array here\n",
    "                        if not check_existing_chunks((t_ind, z_ind, y_ind, x_ind), chunk_ratios, existing_chunks):\n",
    "                            chunk_dict[(t_ind, z_ind, y_ind, x_ind)] = \\\n",
    "                                load_chunk(filename, t_range, z_range, y_range, x_range, scene=scene)\n",
    "                            dask.distributed.print(f\"Added {(t_ind, z_ind, y_ind, x_ind)}\")\n",
    "                        x_ind += 1\n",
    "                    y_ind += 1\n",
    "                z_ind += 1\n",
    "            t_ind += 1\n",
    "    return chunk_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b724c4f-9257-4cf2-b068-c192bf37af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_dims(file_values, chunk_sizes):\n",
    "    \"\"\"\n",
    "    Work out how many chunks in each dimension, return\n",
    "    a TZYX with these values.\n",
    "\n",
    "    Param:\n",
    "    file_values: TCZYX The dimensions of the file.\n",
    "    chunk_sizes: TZYX Chunk sizes.\n",
    "\n",
    "    Returns:\n",
    "    TZYX: How many chunks in each dimension.\n",
    "    \"\"\"\n",
    "    ch_shape = [math.ceil(f/c) for (f, c) in zip(file_values.TZYX, chunk_sizes)]\n",
    "    return TZYX(*ch_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c08d614-a227-416c-9fc6-e3cb62c56022",
   "metadata": {},
   "source": [
    "## The client code\n",
    "Everything beyond this point is expected to run in the Dask client only.\n",
    "\n",
    "First we have all the parameters we need to set to process this image.\n",
    "\n",
    "| Parameter | Note |\n",
    "|-----------|------|\n",
    "|`filename` | Path to CZI input file |\n",
    "| `?_chunk_size` | Size of Dask chunks in the TZYX axes |\n",
    "| `overlaps` | The overlaps, in pixels, on each axis (TZYX) |\n",
    "| `output_filename` | The output Zarr array filename. By default this will be a directory |\n",
    "| `output_chunks` | If creating a new Zarr array, this will be its chunk sizes |\n",
    "\n",
    "Note that the Dask chunks must be integral multiples of the Zarr chunks for this simple workflow\n",
    "(checked below).\n",
    "\n",
    "The Zarr chunks should be small-ish.  In particular there's an issue where if the Zarr\n",
    "chunk is >2GiB then the default compressor will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ea4eb-8e4e-47b6-a635-643f49af0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "# Input file\n",
    "# filename = \"/rds/project/rds-1FbiQayZlSY/data/Millie/Timepoint3-02-Embryo3-Lattice Lightsheet.czi\"\n",
    "# filename = \"/rds/project/rds-1FbiQayZlSY/data/Millie/Millie 24Aug23 raw light sheet data/Timepoint3-02.czi\"\n",
    "filename = \"../../../data/2021-02-25-tulip_Airyscan.czi\"\n",
    "scene = 0\n",
    "# Dask chunks\n",
    "t_chunk_size = 1\n",
    "z_chunk_size = 7\n",
    "y_chunk_size = 250\n",
    "x_chunk_size = 250\n",
    "# Overlaps\n",
    "overlaps = TZYX(0, 0, 16, 16)\n",
    "# Output file\n",
    "# output_filename = \"/rds/project/rds-1FbiQayZlSY/data/Millie/Timepoint3-denoised-notover\"\n",
    "output_filename = \"/tmp/denoised_zarr\"\n",
    "output_chunks=TCZYX(1, 1, 1, 250, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbeccc3-0a09-4b62-a82b-3f4da4a7efd7",
   "metadata": {},
   "source": [
    "Some diagnostic info about the image, then compute the number of chunks in\n",
    "each axis. Note that the \"last\" chunk in each axis may be smaller than the\n",
    "chunk size. Take care that the denoising algorithm can cope with these small\n",
    "chunks (i.e. I think it fails if the image is only a few pixels wide in\n",
    "an axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65a7b2-07a3-48cc-aae1-3898ca531730",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sizes = TZYX(t_chunk_size, z_chunk_size, y_chunk_size, x_chunk_size)\n",
    "czi_file = czi.CziReader(filename)\n",
    "file_array = get_czi_scene_shape(filename, scene)\n",
    "print(f\"scenes_bounding_rectangle: {czi_file.scenes_bounding_rectangle}\")\n",
    "print(f\"\\ntotal_bounding_box: {czi_file.total_bounding_box}\")\n",
    "\n",
    "chunk_array = get_chunk_dims(file_array, chunk_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401de98-4df1-4045-a5c1-930d6bd7b6bd",
   "metadata": {},
   "source": [
    "We try to open the Zarr array for reading. If this fails then we\n",
    "create and open a new Zarr array instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca992969-d10d-4ca9-b5c8-f686525966fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of output array.  Open it if it exists to get\n",
    "# list of existing blocks.\n",
    "try:\n",
    "    output_zarr = zarr.open_array(\n",
    "        output_filename,\n",
    "        mode=\"r\",\n",
    "    )\n",
    "except zarr.errors.ArrayNotFoundError:\n",
    "    output_zarr = zarr.create(\n",
    "        store=output_filename,\n",
    "        shape=file_array,\n",
    "        dtype=np.uint16,\n",
    "        chunks=output_chunks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae2904-028e-4924-ae3c-05ac3527d6f1",
   "metadata": {},
   "source": [
    "We make some simple checks on relative chunk sizes here. Note that these\n",
    "are not *required* by the workflow, but some extra care or coding would\n",
    "need to be in place if these checks are not true.\n",
    "\n",
    "What we're trying to avoid is any Dask chunk sharing a Zarr chunk with\n",
    "another Dask chunk.\n",
    "\n",
    "Let's look at an example where we have a single axis image of 1000 pixels.\n",
    "We split it into 10 Dask chunks of size 100 pixels.\n",
    "We create our output Zarr array with chunks of 50 pixels.  This is fine,\n",
    "because each Dask chunk completely fits into two Zarr chunks, and no Zarr\n",
    "chunk holds more than one Dask chunk.  If we made our Zarr chunks 30 pixels\n",
    "then we'd have a problem because Zarr chunk 3 would span Dask chunks 0 and 1.\n",
    "When we come to write the Dask chunks they may overwrite each other's data\n",
    "in Zarr chunk 3.\n",
    "\n",
    "To avoid this we can ensure the Dask chunks are integral multiples\n",
    "of the Zarr chunk sizes. If this is not desirable or possible we can\n",
    "create the Zarr array with process synchronization as per [the Zarr\n",
    "docs](https://zarr.readthedocs.io/en/support-v2/tutorial.html#parallel-computing-and-synchronization).\n",
    "This will work as long as the underlying filesystem supports file\n",
    "locking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc00ad-96a4-4f77-80cd-d7f60b3cfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sizes and that zarr chunks are integral multiples of dask chunks.\n",
    "# Might also be worth checking that the dask array will fit in the zarr array!\n",
    "output_zarr_chunks = TCZYX(*output_zarr.chunks)\n",
    "for f_ch, d_ch in zip(output_zarr_chunks.TZYX, chunk_sizes):\n",
    "    if f_ch > d_ch:\n",
    "        print(chunk_sizes)\n",
    "        print(output_zarr.chunks)\n",
    "        raise ValueError(\"Dask chunks smaller than output chunks\")\n",
    "    if d_ch % f_ch != 0:\n",
    "        print(chunk_sizes)\n",
    "        print(output_zarr.chunks)\n",
    "        raise ValueError(\"Dask chunks not multiples of output chunks\")\n",
    "        \n",
    "# Get ratios of dask to file chunks.\n",
    "chunk_ratios = TZYX(*[ d_c // f_c for (d_c, f_c) in zip(chunk_sizes, output_chunks.TZYX) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc707f-85db-49a5-a4e7-2aafb786dfa8",
   "metadata": {},
   "source": [
    "We examine the Zarr array for already written (Zarr) chunks. A nice feature of Zarr\n",
    "is that if a chunk is present in the array then it will be complete.  That is,\n",
    "there are no half-written chunks in the array.  That isn't to say that\n",
    "all the Zarr chunks that comprise a Dask chunk are present! We nake a `set`\n",
    "of the chunk indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd971b-a3c6-4bf5-ab86-431b2434ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make set of existing chunks\n",
    "existing_chunks = {\n",
    "    TCZYX(*map(int, k.split('.'))).TZYX for k in output_zarr.store.keys() if k != \".zarray\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fdb047-da32-4d38-8e4b-bcefdb4092b9",
   "metadata": {},
   "source": [
    "### Start creating the Dask task graph\n",
    "\n",
    "We first call `load_chunks_from_czi` which returns a `dict`\n",
    "with keys of Dask chunk indices. The values are the Delayed\n",
    "return values from `load_chunk`, which will eventually be\n",
    "`ndarray`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67a08f-291f-436b-abb7-0b2efb393c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chunks.\n",
    "data_array = load_chunks_from_czi(filename, chunk_sizes, overlaps,\n",
    "                                  existing_chunks, chunk_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef4244-e34e-4a6a-8310-8e2cf7e5cdd8",
   "metadata": {},
   "source": [
    "Now the next steps are similar. We just loop over the items in the\n",
    "dict and perform computations on them. As our computations\n",
    "(`process_block`, `trim`, and `save_block_to_zarr`) are all\n",
    "`@dask.delayed` this builds up the task graph without running\n",
    "the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce162491-bf4e-4909-9ec6-6b36987fb61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoise each chunk.\n",
    "blocked_array = {\n",
    "        k: process_block(v) for (k, v) in data_array.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c5b03-01e1-4ecb-a760-5316de9ad534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the overlap from the chunks.\n",
    "trimmed_array = {\n",
    "    b_ind: trim(block, b_ind, chunk_array, overlaps)\n",
    "        for (b_ind, block) in blocked_array.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cb51c-0c2b-4721-a82b-d43643bd2966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to the output zarr.\n",
    "final_result = [\n",
    "    save_block_to_zarr(block, output_filename, b_ind, chunk_sizes)\n",
    "        for (b_ind, block) in trimmed_array.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049582ed-7e6d-438e-8bba-dcf79b3f351f",
   "metadata": {},
   "source": [
    "Finally we end up with a collection of Delayed objects. We pass these to `dask.compute`\n",
    "_en masse_, which submits them _all_ to the workers for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a6fbf-2cb8-4478-873d-290cad4648a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the actual calculation!  Final result will be an array\n",
    "# of `None`, one for each chunk.\n",
    "dask.compute(*final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaeb9d8-91cc-4eca-90a8-0b5e801577a5",
   "metadata": {},
   "source": [
    "## Extending this work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723c6e2-6ca6-4b52-9492-7104f497a342",
   "metadata": {},
   "source": [
    "The above steps from \"Start creating the Dask task graph\" form the backbone of\n",
    "the Dask workflow. You can insert extra steps, or substitute existing ones.\n",
    "\n",
    "For example, if we decide to use some other input file format instead\n",
    "of CZI we can make some minor edits to `load_chunks_from_czi` to remove\n",
    "the reading of CZI file sizes and\n",
    "re-implement `load_chunk` so it reads the new file format and returns\n",
    "a Delayed `ndarray` corresponding to the given ranges.\n",
    "One thing to note here is that we're passing around\n",
    "a _filename_ rather than a Python `File` type (or similar). This is because\n",
    "each worker may be on a different node, and one machine's file descriptor\n",
    "or file handle will not mean anything on another machine. Only filepaths\n",
    "are truly portable here. You have to imagine that a worker has just\n",
    "been given `load_chunk` with its parameters and no other context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37199b55-9772-4671-a516-75287729f054",
   "metadata": {},
   "source": [
    "The workflow doesn't have to be linear. We can branch off at any point.\n",
    "\n",
    "For example, let's say we want to perform two computations on our chunks\n",
    "once we've loaded them.  Perhaps the second job needs to know\n",
    "the (Dask chunk) index of the block. We might do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab702e1f-cff5-4779-9760-44738d90eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chunks.\n",
    "data_array = load_chunks_from_czi(filename, chunk_sizes, overlaps,\n",
    "                                  existing_chunks, chunk_ratios)\n",
    "# Denoise each chunk.\n",
    "blocked_array = {\n",
    "        k: process_block(v) for (k, v) in data_array.items()\n",
    "}\n",
    "# Do some other work on each chunk. <<- NEW!\n",
    "other_work_array = {\n",
    "    k: other_work(k, v) for (k, v) in data_array.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1922ec-8811-40eb-81e0-3e1771b2e4df",
   "metadata": {},
   "source": [
    "We could then save `other_work_array` in the same way we do `final_result`, or\n",
    "combine its chunks with `blocked_array`, or whatever we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a078b-ecdf-4243-b082-eaaec91b4a67",
   "metadata": {},
   "source": [
    "Or we might decide that treating the input image as a `Dask.Array` is more useful. This is the approach\n",
    "we initially used, with `map_overlap`. The above Delayed approach is more general, but the Array approach\n",
    "has the advantage that each chunk is now explicitly\n",
    "a part of an overall array. However Dask expects you to use more array-like functions to process\n",
    "your data.\n",
    "\n",
    "One approach to do this was to leave the `load_chunk` function unchanged (returning a Delayed `ndarray`)\n",
    "but have the `load_chunks_from_czi` do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b2480-4f7b-4ad2-bd47-9cbe782529b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "def load_chunks_from_czi_alternate(filename, t_chunk_size, z_chunk_size, y_chunk_size, x_chunk_size, scene=0):\n",
    "    data_t = []\n",
    "    with czi.open_czi(filename) as input_file:\n",
    "        bbox = input_file.total_bounding_box\n",
    "        C = bbox[\"C\"][1] - bbox[\"C\"][0]\n",
    "        T = bbox[\"T\"][1] - bbox[\"T\"][0]\n",
    "        Z = bbox[\"Z\"][1] - bbox[\"Z\"][0]\n",
    "\n",
    "        bound_rect = input_file.scenes_bounding_rectangle.get(scene, None)\n",
    "        if not bound_rect:\n",
    "            bound_rect = input_file.total_bounding_rectangle\n",
    "        X_min = bound_rect.x\n",
    "        width = bound_rect.w\n",
    "        Y_min = bound_rect.y\n",
    "        height = bound_rect.h\n",
    "\n",
    "        for t in range(0, T, t_chunk_size):\n",
    "            data_z = []\n",
    "            for z in range(0, Z, z_chunk_size):\n",
    "                data_y = []\n",
    "                for y in range(Y_min, Y_min + height, y_chunk_size):\n",
    "                    data_x = []\n",
    "                    for x in range(X_min, X_min + width, x_chunk_size):\n",
    "                        x_data_array = da.from_delayed(\n",
    "                            load_chunk(\n",
    "                                filename,\n",
    "                                range(t, min(T, t + t_chunk_size)),\n",
    "                                range(z, min(Z, z + z_chunk_size)),\n",
    "                                range(y, min(Y_min + height, y + y_chunk_size)),\n",
    "                                range(x, min(X_min + width, x + x_chunk_size)),\n",
    "                                scene=scene,\n",
    "                            ),\n",
    "                            shape=(\n",
    "                                min(t_chunk_size, T-t),\n",
    "                                C,\n",
    "                                min(z_chunk_size, Z-z),\n",
    "                                min(y_chunk_size, Y_min + height - y),\n",
    "                                min(x_chunk_size, X_min + width - x),\n",
    "                            ),\n",
    "                            meta=np.array((), dtype=np.uint16),\n",
    "                        )\n",
    "                        data_x.append(x_data_array)\n",
    "                    y_data_array = da.concatenate(data_x, axis=4)\n",
    "                    data_y.append(y_data_array)\n",
    "                z_data_array = da.concatenate(data_y, axis=3)\n",
    "                data_z.append(z_data_array)\n",
    "\n",
    "            \n",
    "            t_data_array = da.concatenate(data_z, axis=2)\n",
    "            data_t.append(t_data_array)\n",
    "    return da.concatenate(data_t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89834e-d5d2-4579-8d5e-77beceec10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_array = load_chunks_from_czi_alternate(filename, t_chunk_size, z_chunk_size, y_chunk_size, x_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a6273-aca2-4fdd-9312-1c5592d148f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4346272-cb23-43a0-b99f-3aefdcb42453",
   "metadata": {},
   "source": [
    "This uses `dask.array.from_delayed` to create a new dask Array from each\n",
    "Delayed `ndarray`, then concatenates these into the whole array. Note this\n",
    "Array is never stored in one place!\n",
    "\n",
    "We can now use `map_overlap`, `map_blocks`, or `reduction`, and so on.  E.g.\n",
    "the original workflow used `map_overlap` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439988f-3d5d-4769-a913-7423dd1a0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = (0, 0, 1, 1, 1)\n",
    "blocked_array_alternate = dask_array.map_overlap(\n",
    "    process_block,        # name of function to perform on each chunk\n",
    "    depth=depth,          # the overlaps\n",
    "    boundary=\"none\",      # do nothing special at the boundaries\n",
    "    allow_rechunk=False,  # leave chunking as is\n",
    "    meta=np.array((), dtype=np.float32), # an example of the type of data that will be returned\n",
    "    name=\"process_block\", # a descriptive name to help us\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0b8fc-2d9a-492c-9976-94fca258aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_array_alternate.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4b8a9-c9ce-4263-b020-eec7e9280cfa",
   "metadata": {},
   "source": [
    "An important warning here is that this code won't run! You will need to\n",
    "redefine `process_block` _without_ the `@dask.delayed` decorator.\n",
    "This is because `map_overlap` (and `map_blocks`, etc) run the\n",
    "function they're given as Delayed already, so if you give them a\n",
    "delayed function you get double delayed, which gives unhelpful\n",
    "errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc8baa-8b54-4b8f-bbb7-cfe7e2e04bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
